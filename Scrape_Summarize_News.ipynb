{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scrape_Summarize_News.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUdchKX8B4Eb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Desc: It is for scraping and summarizing news articles from ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIAb51WxCK6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries\n",
        "import nltk\n",
        "from newspaper import Article"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKypW-0aCQGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Article\n",
        "url = 'https://medium.com/@elvinaqa/deep-learning-and-computer-vision-basics-i-86acab09e3b7'\n",
        "article = Article(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWFrX4bsCWO0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6b27c5f6-47a8-4c54-8c9e-d7892150b0dd"
      },
      "source": [
        "# NLP\n",
        "article.download()\n",
        "\n",
        "article.parse()\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "article.nlp()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93MRIoMwDJOx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bedd8e81-c97c-4e76-e6a8-0bef643638cc"
      },
      "source": [
        "# Author\n",
        "article.authors"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Elvin Agammed']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxhwdXwBDSiX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5904229-9377-4547-f428-620a237afb9a"
      },
      "source": [
        "# Date of publish\n",
        "article.publish_date"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.datetime(2020, 2, 16, 20, 27, 35, 534000, tzinfo=tzlocal())"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xiM4IMJDaWS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8bc43d06-cc51-4144-b0b1-6d9863ddf547"
      },
      "source": [
        "# Image used in article\n",
        "article.top_image"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://miro.medium.com/max/1200/1*VFiSi5glQ6g5Bs4LH-arKw.jpeg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJlimwssDhIg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8225b504-c911-415a-cbcb-2f5bb4e7f1f2"
      },
      "source": [
        "# Text of article\n",
        "print(article.text)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I. General Intro\n",
            "\n",
            "In today’s fast-moving world, Machine Learning, Computer Vision and their applications are advancing so rapidly that people trying to acquire them are not able to catch up with. It is continuously evolving and especially for beginners, it is extremely hard to dive into these fancy-sounded areas to specialize in. Things sometimes get so complicated when tons tutorials are offering to teach those subjects, however, fail to make the users understand what they are trying to teach by being too fast, too theoretical, or offering broken setup, installation and outdated code examples. In this article, you will be given a brief introduction to Computer Vision and Deep Learning while having some applications with Python on example datasets.\n",
            "\n",
            "Deep Learning is one of the Machine Learning subfields to teach computers complicated patterns and representations in data. To give you a basic understanding, the brain of the toddler can be shown. After the baby is shown a bunch of images of cats and dogs which teach (train) him, he becomes to be able to differentiate between a dog and a cat. That is effectively what Deep Learning is, as much as it sounds fancy. When it comes to Computer Vision, it is how computers “see” and understand what is around them, just like we see in the movie “Terminator 2”, perceiving the environment, detecting objects according to the pre-trained feature list, etc.\n",
            "\n",
            "A shoot from “Terminator 2” movie\n",
            "\n",
            "Even though CV is not definitely on its best level yet due to the errors in object detection algorithms, its applications can genuinely revolutionize the world in foreseeable future, such as Autonomous Vehicle (self-driving cars), Robotics Surgery, Intelligent Surveillance, Advanced Image, and Video Searching.\n",
            "\n",
            "An example to illustrate detection algorithms\n",
            "\n",
            "This is a practical illustration of how computers see images which is pretty far cry from how incredibly good we see the images. Storing images and understanding of them in the computer’s “eyes” is about the combination of several features such as colors, hues, brightness, and saturation. Specifically, in terms of colors, RGB is the most commonly used color code model where each pixel in an image represents the brightness of red, green, and blue from 0 to 256. This also explains the fact that images are stored in 3-D arrays; each dimension representing values of r, g, b. in terms of brightness (however, grayscale image, black and white images are stored on 2-D arrays). Especially due to the useless noise, viewpoint variations, lighting conditions, scale issues, non-rigid deformations (different distance to the object), occlusion (objects being partially hidden), cluttered views, object class variations (ex: beds being in different shapes for different brands), ambiguous optical illusions (ex: 3D street art) and poor quality cameras with low resolutions, things are getting worse than ever. However, it is definitely advantageous to have 48 MP (108 MP in S20) cameras which are currently being in trends for the brand-new smart-phone market in terms of CV applications.\n",
            "\n",
            "A basic example of how computer’s see images\n",
            "\n",
            "Directly diving into applications, one of the most commonly used libraries is OpenCV whose core is written in C++ (works utilizing wrapper inside Python). The last version of it is OpenCV 4 and it has open-source algorithms of more than 2500 including object detection and tracking algorithms. However, it is definitely recommended to start off with basics to go to the advanced topics slowly.\n",
            "\n",
            "II. Basic Operations -Image Processing\n",
            "\n",
            "If you are reading this, you have surely come across that green rectangle drawn to the detected face from the posts of your cool “Deep guys”. Drawing figures on images is straightforward with the help of OpenCV where it takes only one line of core code to implement.\n",
            "\n",
            "import cv2\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "img_rbg = cv2.imread('../DATA/dog_backpack.jpg')\n",
            "\n",
            "img_rbg = cv2.cvtColor(img_rbg, cv2.COLOR_RGB2BGR)\n",
            "\n",
            "pt1 = (200, 380)\n",
            "\n",
            "pt2 = (600, 700)\n",
            "\n",
            "cv2.rectangle(img_rbg, pt1, pt2, color = (0, 255, 0), thickness = 10)\n",
            "\n",
            "plt.imshow(img_rbg)\n",
            "\n",
            "If you are planning to learn more about CV and DL, surely you have to acquire basic knowledge of some ML models and Data Visualization technics. In the code above, first, we imported OpenCV (cv2) for reading the image and drawing the rectangle on it and then, included, matplotlib library which is for later display the edited image. After reading the image from our local directory with imread() function, we converted the color mode of the image from RGB to BGR, since OpenCV reads images in BGR mode. Then, cv2.rectangle() function takes the arguments of the image to be drawn on, pt1 — first point expressing top-left of the rectangle, pt2- bottom right point of the rectangle. Obviously, (0, 255, 0) expresses the color of the line, in our case, it is Green (RGB). The points can be determined after on the image, or simply drawing can be controlled with the mouse movement as well in the more complex examples which I am putting the code of here.\n",
            "\n",
            "Basic Application on an image\n",
            "\n",
            "Blending and Pasting Images\n",
            "\n",
            "Many built-in functions can be used to blend and mix images based on certain values for each one. In this particular example, we can use addWeighted() function which takes “weight” for each image and mixes them according to the more or less value for each one. However, one constraint in this particular example is about the images strictly being in the same size and shape.\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "%matplotlib inline\n",
            "\n",
            "import cv2\n",
            "\n",
            "img1 = cv2.imread('../DATA/dog_backpack.png')\n",
            "\n",
            "img2 = cv2.imread('../DATA/watermark_no_copy.png') img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
            "\n",
            "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB) img1 =cv2.resize(img1,(1200,1200))\n",
            "\n",
            "img2 =cv2.resize(img2,(1200,1200)) blended = cv2.addWeighted(src1=img1,alpha=0.7,src2=img2,beta=0.3,gamma=0) plt.imshow(blended)\n",
            "\n",
            "After reading and color-correcting the images, as mentioned above, images need to be resized to be in the same shape so that we can use addWeighted function to blend images, according to the values of alpha, beta, and gamma (img1*alpha + img2*beta + gamma). In this particular example, taking given values for weights into consideration, the below picture appears.\n",
            "\n",
            "Blended images example\n",
            "\n",
            "Blurring and Smoothing\n",
            "\n",
            "These functions can mostly be used the removal of noise in images so that to focus on the main, important part to process. At first sight, we may think that rather than blurring, we may even need sharpening the images. However, blurring is mostly related to the edge detection algorithms for high-resolution images where most of them detect too many useless edges in case of not using blurring methods. In this example, we have written text over an image of bricks. Just to show the application, the usage of the median blur is shown which will blur the background image of bricks, and eventually, make the text “bricks” more readable on the image.\n",
            "\n",
            "import cv2\n",
            "\n",
            "img = load_img()\n",
            "\n",
            "font = cv2.FONT_HERSHEY_COMPLEX\n",
            "\n",
            "cv2.putText(img, text = 'bricks', org = (10, 600), fontFace = font, fontScale = 10, color = (255, 0, 0), thickness = 4) # median blur\n",
            "\n",
            "# Good at removing noise and keep the certain details checked\n",
            "\n",
            "median_result = cv2.medianBlur(img, 5)\n",
            "\n",
            "display_img(median_result)\n",
            "\n",
            "In the code above, we have written text onto the image after we loaded the image with our function. Then, the image is blurred with the medianBlur function of OpenCV. The function also takes the parameter of the kernel, which expresses (5,5) kernel in this particular example. Median blurring is good at removing the noise and keeping the details in certain areas of the image. As shown in the output below, the word brick is made to be appeared sharper, by fading out the background image of bricks.\n",
            "\n",
            "Median blur application on an example image of bricks\n",
            "\n",
            "Tons of other choices can be opted when you want to blur the image and increase the efficiency of your CV algorithm including Gaussian blur, filter2D() and blur() built-in functions of OpenCV.\n",
            "\n",
            "III. Basic Video Applications\n",
            "\n",
            "We talked about the basic applications on images, while applications of all for the frames in videos is exactly what the video processing is. First, we will try to connect our webcam and start streaming from there, so later we can either save the recorded video file to local or apply detection algorithms.\n",
            "\n",
            "import cv2\n",
            "\n",
            "cap = cv2.VideoCapture(0) while True:\n",
            "\n",
            "\n",
            "\n",
            "ret, frame = cap.read()\n",
            "\n",
            "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
            "\n",
            "cv2.imshow('frame',gray)\n",
            "\n",
            "\n",
            "\n",
            "if cv2.waitKey(1) & 0xFF == ord('q'):\n",
            "\n",
            "break\n",
            "\n",
            "cap.release()\n",
            "\n",
            "cv2.destroyAllWindows()\n",
            "\n",
            "Code above, as usual, starts with importing OpenCV, while VideoCapture function reads from our webcam (0) and returns it to the cap. After, we infinitely read the frame and save it to the frame variable by tuple unpacking, which read() function returns. All processing methods for images can be applied to the frame since the frame plays the role of an image in the while loop here. Since we infinitely read and return frames, we should also be able to end the recording process by breaking the while loop when Escape is pressed. In the end, we should release the camera trigger and destroy the current windows and close. OpenCV has broad functionalities especially in terms of video processing as well, where we did not mention saving videos to local, reading local files by slowing down the speed of frame coming and going, or drawing figures on frames when detecting face which we will cover in the following paragraphs.\n",
            "\n",
            "Object Detection Methods\n",
            "\n",
            "Detecting objects in videos is one of the most common problems in the trends of Computer Vision projects. It is mostly done with either drawing a bounding box when an aimed object is identified or coloring the pixels of a particular area for showcasing the detected field of the image. Several methods are more widely used than others, which we will slightly cover some.\n",
            "\n",
            "Template Matching- is one of the simplest forms of object-detection where the algorithm scans for the smaller image in the area of the larger image. It identifies the smaller image on the entire image by sliding the little one over the bigger. One simple detail about template matching is about the sizes to have the exact same value. Let’s say if the smaller image has 400, 500, 3 dimensions, the bigger image should contain these values inside of it. Let’s directly look at the application of it.\n",
            "\n",
            "import cv2\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "%matplotlib inline full = cv2.imread('../DATA/sammy.jpg')\n",
            "\n",
            "full = cv2.cvtColor(full, cv2.COLOR_BGR2RGB)\n",
            "\n",
            "face = cv2.imread('../DATA/sammy_face.jpg')\n",
            "\n",
            "face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB) methods = ['cv2.TM_CCOEFF', 'cv2.TM_CCOEFF_NORMED', 'cv2.TM_CCORR','cv2.TM_CCORR_NORMED', 'cv2.TM_SQDIFF', 'cv2.TM_SQDIFF_NORMED'] for m in methods:\n",
            "\n",
            "full_copy = full.copy()\n",
            "\n",
            "method = eval(m) res = cv2.matchTemplate(full_copy,face,method)\n",
            "\n",
            "\n",
            "\n",
            "min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
            "\n",
            "\n",
            "\n",
            "if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
            "\n",
            "top_left = min_loc\n",
            "\n",
            "else:\n",
            "\n",
            "top_left = max_loc\n",
            "\n",
            "\n",
            "\n",
            "bottom_right = (top_left[0] + width, top_left[1] + height) cv2.rectangle(full_copy,top_left, bottom_right, (0,255,0), 10) plt.subplot(121)\n",
            "\n",
            "plt.imshow(res)\n",
            "\n",
            "\n",
            "\n",
            "plt.subplot(122)\n",
            "\n",
            "plt.imshow(full_copy)\n",
            "\n",
            "\n",
            "\n",
            "plt.show()\n",
            "\n",
            "print('\n",
            "\n",
            "')\n",
            "\n",
            "After having the images read, we have several options to use when applying the template matching to an image as listed in the “methods” array. You can go through each one to see the difference between their results and accuracies. For instance, when we apply the first method, which is ‘cv2.TM_CCOEFF’, we will get the heatmap where it has higher values where the algorithm thinks the second image is.\n",
            "\n",
            "Template Matching example image\n",
            "\n",
            "I am putting the pictures here, where the first image shows the heatmap of the probability of the third image being in the second image in terms of location. After getting the max and min values for the heatmap, we can also draw a rectangle to see the dog’s face more clearly.\n",
            "\n",
            "Corner Detection\n",
            "\n",
            "When talking about corner detection, one first should know what the corner is and how is it identified. It can be said as the point of junction of two edges, where the brightness value of that part of the image is higher. The basic idea in corner detection is about the sharp distinction in the color of pixels when going through images. Several algorithms are used for corner detection, such as Harris Corner Detection, Shi-Tomasi algorithm, however, we will show the application of the first one. Harris algorithm is first introduced back than 1988 and mostly applied to the CV problems for solving corner detection and feature inferring problems.\n",
            "\n",
            "Since chess boards are the perfect examples of images having a bunch of corners, we will use it for applying all, corner, edge and grid detection algorithms on it. Since we talked about them, I will not include a detailed explanation for imports and basic preprocessing of imported images, such as naming them or converting colors of it by playing with color modes. Without prior knowledge of kernels, image processing methods, such as thresholding, dilating, etc, this section can be a little confusing. Thus, it is highly recommended to go further after acquiring the prior examples and researching for more image processing.\n",
            "\n",
            "gray = np.float32(gray_real_chess) dst = cv2.cornerHarris(src=gray,blockSize=2,ksize=3,k=0.04) real_chess[dst>0.01*dst.max()]=[255,0,0] plt.imshow(real_chess)\n",
            "\n",
            "Since the Harris corner detection algorithm accepts images in the form of floating-point numbers, we will first convert it to float32 with the help of the NumPy function. Again, OpenCV makes things easier than ever for us, in this particular example, it takes the cornerHarris function and feeds source image, blocksize for the size of the considered neighborhood for detection, ksize for Sobel derivative, and k which is a free parameter in the function(read more from “cornereigenvalsvecs”).\n",
            "\n",
            "Harris Corner Application example\n",
            "\n",
            "Edge Detection\n",
            "\n",
            "For edge detection, we will use the Canny edge detector, which will be broken down into steps before seeing the actual results for the detected areas. First, we need to apply the Gaussian filter to smooth the images and remove the noise. As mentioned earlier, without removing the noise from an image, it is definitely complicated to determined edges or grids, since the algorithm will probably “underfit” by omitting some edges, or “overfit” by detecting way more edges than there actually is. For the Canny detector, we also need to supply the details for the upper and lower threshold values to partially eliminate the foregoing issue.\n",
            "\n",
            "import cv2\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "%matplotlib inline img = cv2.imread('../DATA/sammy_face.jpg') edges = cv2.Canny(image=img, threshold1=127, threshold2=127)\n",
            "\n",
            "plt.imshow(edges)\n",
            "\n",
            "After import the libraries and reading images, as mentioned, we are using the canny algorithm to detect the edges. It takes the parameters of the source image and threshold values for lower and upper bounds which we will randomly choose in this particular example to not dive deep into the explanation of the perfect values for thresholds here.\n",
            "\n",
            "Edge Detection example\n",
            "\n",
            "Grid Detection\n",
            "\n",
            "Grid detection is widely used in most of today’s cameras to calibrate themselves and eliminate the distortion in images. You have probably already known about the reason for the usage of that box detection when taking everyday selfies of you. It is for the calibration and removing the distortion when the camera is moving on your hands. Thus, a rectangle is attached to your face so that it can keep the concentration of the main point (which is your face in this particular example) in the image and take more quality pictures. As always, OpenCV has solutions to each one of those “complicated” problems, and for detecting the grid in the chess boards, we will use its built-in grid detection function. One which we implement\n",
            "\n",
            "import cv2\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "%matplotlib inline flat_chess = cv2.imread('../DATA/flat_chessboard.png') found, corners = cv2.findChessboardCorners(flat_chess,(7,7)) cv2.drawChessboardCorners(flat_chess, (7, 7), corners, found)\n",
            "\n",
            "plt.imshow(flat_chess)\n",
            "\n",
            "Shown example especially designed to look for the chessboard-like designs, thus, it is not performing perfectly in any other non-related shapes when applying. Here, we are using the findChessBoardCorners() function which takes the parameters of the source image and the size for the grid. In our image which you see below, we have 8 to 8 grid, obvious in any other boards, however, the function is not able to read the outer edges since they are not complete to the square and it is where the image ends. Thus, this is the reason why we are using a 7,7 grid. The function will get us 2 values which we will retrieve from tuple unpacking, first is found-a boolean value returning true when finding the grid-like pattern and corners -a list returning the list of the array of the location for the corners detected. Now, since we have the locations for the detected parts, we can direct the values to another built-in method of cv, which is drawChessboardCorners() which draws the colored line between detected points as seen below example.\n",
            "\n",
            "Grid Detection example\n",
            "\n",
            "Face Detection\n",
            "\n",
            "For the sake of the fact that it is the first introductory article about CV, we will not focus on detecting faces as who they belong to since it requires a large dataset and Deep Learning applications. However, we will apply the Haar Cascade face detection algorithm to detect and locate the face. A common misconception for detecting the face is that algorithm searches through all image and stop when it does “see” the face. However, the truth is, especially in this particular example, the algorithm works according to the sum of pixels difference between the points in the image. It calculates the edges and lines and checks for the features of ex: eyes versus eyebrows, or nose bridge versus sides of the nose so that it can differentiate between them to detect a face. One disadvantage for it is the need for large datasets to train new features if we need to add any additional features to detect. To eliminate this need, OpenCV offers us a built-in cascade classifier which has already been pre-trained.\n",
            "\n",
            "import numpy as np\n",
            "\n",
            "import cv2\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "%matplotlib inline solvay = cv2.imread('../DATA/solvay_conference.jpg',0) plt.imshow(solvay,cmap='gray') face_cascade = cv2.CascadeClassifier('../DATA/haarcascades/haarcascade_frontalface_default.xml') def detect_face(face_img): face_rects = face_cascade.detectMultiScale(face_img)\n",
            "\n",
            "\n",
            "\n",
            "for (x,y,w,h) in face_rects:\n",
            "\n",
            "cv2.rectangle(face_img, (x,y), (x+w,y+h), (255,255,255), 10)\n",
            "\n",
            "\n",
            "\n",
            "return face_img\n",
            "\n",
            "result = detect_face(solvay)\n",
            "\n",
            "plt.imshow(result,cmap='gray')\n",
            "\n",
            "As it is obvious from the detected faces, it is not that perfectly working but still fine it has correctly detected almost 95 percent of all faces. After reading the image, as always it is converted to the grayscale. To use Haar Cascade, we make an object of it, which will be later used to get the function of detectMultiScale() for returning us the values of location (x, y, w, h) for each face in the image. For an additional example to catch between the lines, you can also check plate detection by using Haar cascade for plates from here.\n",
            "\n",
            "Solvay Conference Face Detection\n",
            "\n",
            "Since the topic is broad enough to talk about all at once, I will sum up this one here where we talked about General Computer Vision, specifically, color modes, how a computer reads images, basic image, and video processing applications, while in the end giving some practical knowledge of object detection and tracking technics. In the following article, we will broadly cover topics from Deep Learning for Computer Vision where detailed explanations and example applications on sample datasets, images, and videos will be supplied. The topics of Optical Flow, Meanshift, Camshift and other Tracking APIs from OpenCV, practical and theoretical applications of Keras, CNNs and YOLO v3 will be discussed.\n",
            "\n",
            "Thanks for YOUr time to read it to the end (if you are still proceeding this long one:). If there are any omissions or mistakes, I am always open to constructive criticism. Cheers! </>\n",
            "\n",
            "Elvin Aghammadzada\n",
            "\n",
            "A CS Student, Baku Engineering University\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7eurYdNDpEi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "2159a59e-81e6-4088-d26d-70c2c848953b"
      },
      "source": [
        "# Summarize the article\n",
        "print(article.summary)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In this article, you will be given a brief introduction to Computer Vision and Deep Learning while having some applications with Python on example datasets.\n",
            "Deep Learning is one of the Machine Learning subfields to teach computers complicated patterns and representations in data.\n",
            "Object Detection MethodsDetecting objects in videos is one of the most common problems in the trends of Computer Vision projects.\n",
            "However, we will apply the Haar Cascade face detection algorithm to detect and locate the face.\n",
            "In the following article, we will broadly cover topics from Deep Learning for Computer Vision where detailed explanations and example applications on sample datasets, images, and videos will be supplied.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdwbGSrMD7h-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "89775fc0-296d-4bff-aae7-6768009321e2"
      },
      "source": [
        "print(article.images)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'https://miro.medium.com/max/60/1*6osxfTl1CgyFUJA-OTW08A.png?q=20', 'https://miro.medium.com/max/60/1*KojKkAw4G4QK1h13ddM6og.png?q=20', 'https://miro.medium.com/freeze/max/60/1*PJB_jdKKhNICpLeUt5piFA.gif?q=20', 'https://miro.medium.com/max/1200/1*VFiSi5glQ6g5Bs4LH-arKw.jpeg', 'https://miro.medium.com/max/750/1*4FzrPQ-10OrbgjaFLIpv8g.png', 'https://miro.medium.com/max/60/1*UwuMd_8xQnJClaNjKIkzxw.png?q=20', 'https://miro.medium.com/max/60/1*x3QKYkD41-z-XiyaS6I2oQ.png?q=20', 'https://miro.medium.com/max/552/1*pFpIza5yG1HsrwYP7yWAzA.png', 'https://miro.medium.com/fit/c/160/160/2*-YCD31uEzZpFpLZUj5v1ng.png', 'https://miro.medium.com/max/1400/1*IKiBCS5QnNIeWHxmXlAztA.jpeg', 'https://miro.medium.com/max/60/1*pFpIza5yG1HsrwYP7yWAzA.png?q=20', 'https://miro.medium.com/max/734/1*UwuMd_8xQnJClaNjKIkzxw.png', 'https://miro.medium.com/max/382/1*i0SooIq9-sZvolnZQOCKLA.png', 'https://miro.medium.com/max/60/1*IKiBCS5QnNIeWHxmXlAztA.jpeg?q=20', 'https://miro.medium.com/max/650/1*dOtOomnt0enDte8sKVt2NA.png', 'https://miro.medium.com/max/60/1*dOtOomnt0enDte8sKVt2NA.png?q=20', 'https://miro.medium.com/max/60/1*VFiSi5glQ6g5Bs4LH-arKw.jpeg?q=20', 'https://miro.medium.com/max/1418/1*KojKkAw4G4QK1h13ddM6og.png', 'https://miro.medium.com/fit/c/80/80/1*rje4HdxBpv0iSTS5FKbJrA.jpeg', 'https://miro.medium.com/max/1000/1*PJB_jdKKhNICpLeUt5piFA.gif', 'https://miro.medium.com/max/2560/1*VFiSi5glQ6g5Bs4LH-arKw.jpeg', 'https://miro.medium.com/max/46/1*i0SooIq9-sZvolnZQOCKLA.png?q=20', 'https://miro.medium.com/max/768/1*x3QKYkD41-z-XiyaS6I2oQ.png', 'https://miro.medium.com/fit/c/96/96/2*-YCD31uEzZpFpLZUj5v1ng.png', 'https://miro.medium.com/max/1052/1*bj5LcX88zW9xfWRMkK3PCQ.gif', 'https://miro.medium.com/max/60/1*4FzrPQ-10OrbgjaFLIpv8g.png?q=20', 'https://miro.medium.com/max/514/1*6osxfTl1CgyFUJA-OTW08A.png', 'https://miro.medium.com/fit/c/80/80/2*ZjmT1D-KR0ssWtHTkZN5nQ.jpeg', 'https://miro.medium.com/freeze/max/60/1*bj5LcX88zW9xfWRMkK3PCQ.gif?q=20', 'https://miro.medium.com/fit/c/80/80/1*BKpcLqs19D7t6cgv6N4yvQ.jpeg'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yrKvTKCFAUq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d1c2047-516b-4f41-a6e4-10075652886f"
      },
      "source": [
        "print(article.keywords)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['values', 'image', 'images', 'applications', 'opencv', 'basics', 'learning', 'algorithm', 'detection', 'vision', 'face', 'computer', 'function', 'deep', 'example']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiYp83WFFfvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51b17dcb-c795-450c-a880-67f640ebcf07"
      },
      "source": [
        "print(article.title)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deep Learning and Computer Vision Basics I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YThUmv9JGbhK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48192b00-f2ef-41ec-9976-21de2a4c5943"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1gLIjiDGmQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}